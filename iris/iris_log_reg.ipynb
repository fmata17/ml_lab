{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8a6fec7",
   "metadata": {},
   "source": [
    "<h1 style=\"color:white;\">\n",
    "    Iris Multiclass Logistic Regression\n",
    "</h1>\n",
    "<h3 style=\"color:white;\">\n",
    "    Single Layer Perceptron with Softmax Output Head\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f4c583",
   "metadata": {},
   "source": [
    "<h3 style=\"color:white;\">\n",
    "    Load and split data into training and testing sets\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a98dd406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 150 samples with 4 features each.\n",
      "Training set: 120 samples\n",
      "Testing set: 30 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder  # consider StandardScaler too\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=14)\n",
    "\n",
    "print(f\"Loaded {X.shape[0]} samples with {X.shape[1]} features each.\"\n",
    "      f\"\\nTraining set: {X_train.shape[0]} samples\"\n",
    "      f\"\\nTesting set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2bf73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- One-hot encode targets ---\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_oh = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_oh = encoder.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1519ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def linear_logits(X, weights, bias):\n",
    "    return np.dot(X, weights) + bias  # bias is broadcasted to each sample row\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Row-wise softmax with numerical stabilization.\n",
    "    Z: (N, K) -> P: (N, K), each row sums to 1.\n",
    "    \"\"\"\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # stability trick\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy loss between true labels and predicted probabilities.\n",
    "\n",
    "    Cross-entropy measures the difference between two probability distributions:\n",
    "    the true distribution (`y_true`, typically one-hot encoded) and the predicted\n",
    "    distribution (`y_pred`). A lower loss indicates that the predicted distribution\n",
    "    is closer to the true distribution.\n",
    "\n",
    "    Assumptions:\n",
    "    - `y_true` is a NumPy array of shape (n_samples, n_classes) with one-hot \n",
    "      encoded labels.\n",
    "    - `y_pred` is a NumPy array of shape (n_samples, n_classes) containing predicted\n",
    "      probabilities (each row should sum to 1).\n",
    "    - NumPy (`np`) is imported and available in the namespace.\n",
    "    - This implementation is intended for multi-class classification problems.\n",
    "    \"\"\"\n",
    "    # Clip predicted values to avoid log(0), which would cause NaN/inf errors\n",
    "    y_pred_clipped = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "\n",
    "    # Compute negative log-likelihood for each sample\n",
    "    log_probs = -np.sum(y_true * np.log(y_pred_clipped), axis=1)\n",
    "\n",
    "    # Return average cross-entropy across all samples\n",
    "    return np.mean(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0e65ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      "[[ 0.01551339  0.00079186  0.00173977]\n",
      " [-0.00072337 -0.02004329  0.00144678]\n",
      " [-0.01501169  0.00211109 -0.00558205]\n",
      " [ 0.01084529 -0.00186289  0.00014661]]\n",
      "\n",
      "Biases:\n",
      "[-1.07556947  0.64225207 -0.18033671]\n",
      "\n",
      "Initial loss: 1.267801484586637\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(14)\n",
    "weights = np.random.randn(X.shape[1], y_train_oh.shape[1]) * 0.01 # one weight per feature\n",
    "bias = np.random.randn(y_train_oh.shape[1])  # single bias\n",
    "learning_rate = 0.01\n",
    "\n",
    "# initial loss\n",
    "y_pred = linear_logits(X_train, weights, bias)\n",
    "y_pred_prob = softmax(y_pred)\n",
    "loss = cross_entropy(y_train_oh, y_pred_prob)\n",
    "\n",
    "print(f\"Weights:\\n{weights}\\n\")\n",
    "print(f\"Biases:\\n{bias}\\n\")\n",
    "print(f\"Initial loss: {float(loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d59872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.2678\n",
      "Epoch 1000, Loss: 0.3648\n",
      "Epoch 2000, Loss: 0.2776\n",
      "Epoch 3000, Loss: 0.2317\n",
      "Epoch 4000, Loss: 0.2031\n",
      "Epoch 5000, Loss: 0.1834\n",
      "Epoch 6000, Loss: 0.1690\n",
      "Epoch 7000, Loss: 0.1580\n",
      "Epoch 8000, Loss: 0.1492\n",
      "Epoch 9000, Loss: 0.1421\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10000):\n",
    "    y_pred = linear_logits(X_train, weights, bias)\n",
    "    y_pred_prob = softmax(y_pred)\n",
    "    loss = cross_entropy(y_train_oh, y_pred_prob)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    # Gradient calculation\n",
    "    error = y_pred_prob - y_train_oh # softmax and cross-entropy partial derivatives\n",
    "    weights_grad = (1 / X_train.shape[0]) * np.dot(X_train.T, error)\n",
    "    bias_grad = (1 / X_train.shape[0]) * np.sum(error, axis=0)\n",
    "    \n",
    "    # Update weights and bias\n",
    "    weights -= learning_rate * weights_grad\n",
    "    bias -= learning_rate * bias_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e29bbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0798\n",
      "Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = linear_logits(X_test, weights, bias)\n",
    "model = softmax(y_test_pred)\n",
    "\n",
    "test_loss = cross_entropy(y_test_oh, model)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "model_labels = np.argmax(model, axis=1)\n",
    "acc = np.mean(model_labels == y_test)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
