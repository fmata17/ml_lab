{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f27b36a0",
   "metadata": {},
   "source": [
    "<h1 style=\"color:white;\">\n",
    "    Iris Multilayer Perceptron (MLP)\n",
    "</h1>\n",
    "<h3 style=\"color:white;\">\n",
    "    Fully Connected Neural Network<br>\n",
    "    Dense Neural Network\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4773a620",
   "metadata": {},
   "source": [
    "<h3 style=\"color:white;\">\n",
    "    Load and split data into training and testing sets\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "955150a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (120, 4), Test set size: (30, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import Bunch\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # Stratified split to maintain class distribution in train and test sets\n",
    "print(f\"Training set size: {X_train.shape}, Test set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6978cb4",
   "metadata": {},
   "source": [
    "<h3>Step 1 — Initialize Network Parameters</h3>\n",
    "<p>\n",
    "We create the weight matrices and bias vectors for a 1-hidden-layer classifier on Iris.<br>\n",
    "Shapes follow the layer connectivity: <code>X (n_samples, 4)</code> → <code>Hidden (n_samples, 12)</code> → <code>Pred (n_samples, 3)</code>.\n",
    "</p>\n",
    "<ul>\n",
    "  <li><code>W1</code> ∈ ℝ<sup>4×12</sup>: input→hidden weights</li>\n",
    "  <li><code>b1</code> ∈ ℝ<sup>1×12</sup>: hidden biases (broadcast across samples)</li>\n",
    "  <li><code>W2</code> ∈ ℝ<sup>12×3</sup>: hidden→output weights</li>\n",
    "  <li><code>b2</code> ∈ ℝ<sup>1×3</sup>: output biases (broadcast across samples)</li>\n",
    "</ul>\n",
    "<p>\n",
    "We set a fixed random seed for reproducibility. Biases start at zero; weights use a normal draw (you can switch to Xavier/He init later).\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "616ae4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "input_size = 4       # features\n",
    "hidden_size = 12     # you can tweak\n",
    "output_size = 3      # classes\n",
    "\n",
    "# Initialize weights\n",
    "W1 = np.random.randn(input_size, hidden_size)   # [4, 12] weights from input to hidden\n",
    "b1 = np.zeros((1, hidden_size))                 # [1, 12] bias for hidden layer\n",
    "W2 = np.random.randn(hidden_size, output_size)  # [12, 3] weights from hidden to output\n",
    "b2 = np.zeros((1, output_size))                 # [1, 3] bias for output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fea0e8",
   "metadata": {},
   "source": [
    "<h3>Step 2 — One-Hot Encode the Labels</h3>\n",
    "<p>\n",
    "Neural networks expect the targets to be represented in a way that matches the output layer. \n",
    "For the Iris dataset, we have 3 possible classes. Instead of labels like <code>[0, 1, 2]</code>, \n",
    "we convert them into one-hot vectors:\n",
    "</p>\n",
    "<ul>\n",
    "  <li><code>[0] → [1, 0, 0]</code></li>\n",
    "  <li><code>[1] → [0, 1, 0]</code></li>\n",
    "  <li><code>[2] → [0, 0, 1]</code></li>\n",
    "</ul>\n",
    "<p>\n",
    "We use <code>OneHotEncoder</code> to transform <code>y_train</code> and <code>y_test</code> into binary matrices:\n",
    "</p>\n",
    "<ul>\n",
    "  <li><code>y_train_oh</code> → shape (<code>n_train_samples × 3</code>)</li>\n",
    "  <li><code>y_test_oh</code> → shape (<code>n_test_samples × 3</code>)</li>\n",
    "</ul>\n",
    "<p>\n",
    "This ensures the labels align with the network’s output probabilities from the softmax layer.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb138773",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_oh = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_oh = encoder.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f09307",
   "metadata": {},
   "source": [
    "<h3>Step 3 — Define Activations and Forward Pass</h3>\n",
    "<p>\n",
    "We implement the non-linearities and run a forward pass through the network (before training) to inspect the initial outputs:\n",
    "</p>\n",
    "<ul>\n",
    "  <li><code>ReLU(x) = max(0, x)</code> is applied to the hidden layer to introduce non-linearity.</li>\n",
    "  <li><code>Softmax(x)</code> converts the output logits into class probabilities, stabilized by subtracting <code>max(x)</code> to avoid overflow.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae120490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # stability trick\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Forward pass (before training) to check initial loss\n",
    "hidden = relu(np.dot(X_train, W1) + b1)\n",
    "output = softmax(np.dot(hidden, W2) + b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cc6974",
   "metadata": {},
   "source": [
    "<h3>Step 4 — Define Loss Function (Cross-Entropy)</h3>\n",
    "<p>\n",
    "We use <strong>cross-entropy loss</strong>, the standard choice for classification with softmax outputs.\n",
    "It measures how far the predicted probability distribution is from the true one-hot labels:\n",
    "</p>\n",
    "<p style=\"text-align:center;\">\n",
    "  <code>L = - (1/N) Σ (y_true · log(y_pred))</code>\n",
    "</p>\n",
    "<ul>\n",
    "  <li><code>y_true</code>: one-hot encoded labels (shape = N×3)</li>\n",
    "  <li><code>y_pred</code>: predicted probabilities from softmax (shape = N×3)</li>\n",
    "  <li>We apply <code>np.clip(y_pred, 1e-15, 1 - 1e-15)</code> to avoid numerical issues:\n",
    "    <ul>\n",
    "      <li>Prevents <code>log(0)</code> → <code>-∞</code>.</li>\n",
    "      <li>Keeps probabilities in a safe range: [1e-15, 0.999999999999999].</li>\n",
    "      <li>Ensures gradients remain finite during training.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>The function returns the mean loss across all samples.</li>\n",
    "</ul>\n",
    "<p>\n",
    "At this stage, we compute the <em>initial loss</em> before training. A high value is expected since weights are random.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26a50a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 8.495902981763878\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy(y_true, y_pred):\n",
    "    # clip to avoid log(0)\n",
    "    y_pred_clipped = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    log_probs = -np.sum(y_true * np.log(y_pred_clipped), axis=1)\n",
    "    return np.mean(log_probs)\n",
    "\n",
    "loss = cross_entropy(y_train_oh, output)\n",
    "print(\"Initial loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5e7c76",
   "metadata": {},
   "source": [
    "<h3>Step 5 — Training Loop (Forward, Backward, Update)</h3>\n",
    "<p>\n",
    "We train the network using gradient descent for 1000 epochs with a learning rate of 0.01. \n",
    "Each epoch performs three main steps:\n",
    "</p>\n",
    "<ol>\n",
    "  <li>\n",
    "    <strong>Forward pass</strong>:\n",
    "    <ul>\n",
    "      <li>Compute hidden activations: <code>hidden = ReLU(X·W1 + b1)</code></li>\n",
    "      <li>Compute output probabilities: <code>output = softmax(hidden·W2 + b2)</code></li>\n",
    "      <li>Evaluate cross-entropy loss against true labels.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>\n",
    "    <strong>Backward pass (backpropagation)</strong>:\n",
    "    <ul>\n",
    "      <li>Error at output: <code>dZ2 = output - y_true</code></li>\n",
    "      <li>Gradients for output weights/biases: <code>dW2, db2</code></li>\n",
    "      <li>Propagate error back to hidden layer: <code>dH = dZ2·W2<sup>T</sup></code></li>\n",
    "      <li>Apply ReLU derivative: zero out gradients where <code>hidden ≤ 0</code></li>\n",
    "      <li>Gradients for input→hidden weights/biases: <code>dW1, db1</code></li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>\n",
    "    <strong>Parameter update</strong>:\n",
    "    <ul>\n",
    "      <li>Weights and biases are updated with gradient descent:</li>\n",
    "      <li><code>W ← W - η·dW</code>, <code>b ← b - η·db</code></li>\n",
    "      <li>(η = learning rate = 0.01)</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "</ol>\n",
    "<p>\n",
    "We print the loss every 100 epochs to monitor convergence. Over time, \n",
    "the loss should decrease as the network learns to classify the Iris dataset.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95c1c320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 8.4959\n",
      "Epoch 100, Loss: 0.2326\n",
      "Epoch 200, Loss: 0.2037\n",
      "Epoch 300, Loss: 0.1850\n",
      "Epoch 400, Loss: 0.1711\n",
      "Epoch 500, Loss: 0.1601\n",
      "Epoch 600, Loss: 0.1511\n",
      "Epoch 700, Loss: 0.1437\n",
      "Epoch 800, Loss: 0.1375\n",
      "Epoch 900, Loss: 0.1321\n",
      "Epoch 1000, Loss: 0.1275\n",
      "Epoch 1100, Loss: 0.1234\n",
      "Epoch 1200, Loss: 0.1198\n",
      "Epoch 1300, Loss: 0.1165\n",
      "Epoch 1400, Loss: 0.1136\n",
      "Epoch 1500, Loss: 0.1110\n",
      "Epoch 1600, Loss: 0.1086\n",
      "Epoch 1700, Loss: 0.1064\n",
      "Epoch 1800, Loss: 0.1044\n",
      "Epoch 1900, Loss: 0.1026\n",
      "Epoch 2000, Loss: 0.1009\n",
      "Epoch 2100, Loss: 0.0993\n",
      "Epoch 2200, Loss: 0.0978\n",
      "Epoch 2300, Loss: 0.0965\n",
      "Epoch 2400, Loss: 0.0952\n",
      "Epoch 2500, Loss: 0.0940\n",
      "Epoch 2600, Loss: 0.0928\n",
      "Epoch 2700, Loss: 0.0918\n",
      "Epoch 2800, Loss: 0.0908\n",
      "Epoch 2900, Loss: 0.0898\n",
      "Epoch 3000, Loss: 0.0889\n",
      "Epoch 3100, Loss: 0.0881\n",
      "Epoch 3200, Loss: 0.0873\n",
      "Epoch 3300, Loss: 0.0865\n",
      "Epoch 3400, Loss: 0.0858\n",
      "Epoch 3500, Loss: 0.0851\n",
      "Epoch 3600, Loss: 0.0844\n",
      "Epoch 3700, Loss: 0.0838\n",
      "Epoch 3800, Loss: 0.0832\n",
      "Epoch 3900, Loss: 0.0826\n",
      "Epoch 4000, Loss: 0.0820\n",
      "Epoch 4100, Loss: 0.0815\n",
      "Epoch 4200, Loss: 0.0810\n",
      "Epoch 4300, Loss: 0.0805\n",
      "Epoch 4400, Loss: 0.0800\n",
      "Epoch 4500, Loss: 0.0796\n",
      "Epoch 4600, Loss: 0.0791\n",
      "Epoch 4700, Loss: 0.0787\n",
      "Epoch 4800, Loss: 0.0783\n",
      "Epoch 4900, Loss: 0.0779\n",
      "Epoch 5000, Loss: 0.0775\n",
      "Epoch 5100, Loss: 0.0771\n",
      "Epoch 5200, Loss: 0.0768\n",
      "Epoch 5300, Loss: 0.0764\n",
      "Epoch 5400, Loss: 0.0761\n",
      "Epoch 5500, Loss: 0.0757\n",
      "Epoch 5600, Loss: 0.0754\n",
      "Epoch 5700, Loss: 0.0751\n",
      "Epoch 5800, Loss: 0.0748\n",
      "Epoch 5900, Loss: 0.0745\n",
      "Epoch 6000, Loss: 0.0742\n",
      "Epoch 6100, Loss: 0.0739\n",
      "Epoch 6200, Loss: 0.0737\n",
      "Epoch 6300, Loss: 0.0734\n",
      "Epoch 6400, Loss: 0.0731\n",
      "Epoch 6500, Loss: 0.0729\n",
      "Epoch 6600, Loss: 0.0726\n",
      "Epoch 6700, Loss: 0.0724\n",
      "Epoch 6800, Loss: 0.0722\n",
      "Epoch 6900, Loss: 0.0719\n",
      "Epoch 7000, Loss: 0.0717\n",
      "Epoch 7100, Loss: 0.0715\n",
      "Epoch 7200, Loss: 0.0713\n",
      "Epoch 7300, Loss: 0.0711\n",
      "Epoch 7400, Loss: 0.0709\n",
      "Epoch 7500, Loss: 0.0707\n",
      "Epoch 7600, Loss: 0.0705\n",
      "Epoch 7700, Loss: 0.0703\n",
      "Epoch 7800, Loss: 0.0701\n",
      "Epoch 7900, Loss: 0.0699\n",
      "Epoch 8000, Loss: 0.0697\n",
      "Epoch 8100, Loss: 0.0695\n",
      "Epoch 8200, Loss: 0.0693\n",
      "Epoch 8300, Loss: 0.0692\n",
      "Epoch 8400, Loss: 0.0690\n",
      "Epoch 8500, Loss: 0.0688\n",
      "Epoch 8600, Loss: 0.0686\n",
      "Epoch 8700, Loss: 0.0685\n",
      "Epoch 8800, Loss: 0.0683\n",
      "Epoch 8900, Loss: 0.0682\n",
      "Epoch 9000, Loss: 0.0680\n",
      "Epoch 9100, Loss: 0.0679\n",
      "Epoch 9200, Loss: 0.0677\n",
      "Epoch 9300, Loss: 0.0676\n",
      "Epoch 9400, Loss: 0.0674\n",
      "Epoch 9500, Loss: 0.0673\n",
      "Epoch 9600, Loss: 0.0671\n",
      "Epoch 9700, Loss: 0.0670\n",
      "Epoch 9800, Loss: 0.0668\n",
      "Epoch 9900, Loss: 0.0667\n"
     ]
    }
   ],
   "source": [
    "n_samples = X_train.shape[0]\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(10000):\n",
    "    # forward\n",
    "    hidden = relu(np.dot(X_train, W1) + b1)\n",
    "    output = softmax(np.dot(hidden, W2) + b2)\n",
    "\n",
    "    # loss\n",
    "    loss = cross_entropy(y_train_oh, output)\n",
    "\n",
    "    # backward\n",
    "    dZ2 = output - y_train_oh\n",
    "    dW2 = np.dot(hidden.T, dZ2) / n_samples\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / n_samples\n",
    "\n",
    "    dH = np.dot(dZ2, W2.T)\n",
    "    dH[hidden <= 0] = 0\n",
    "    dW1 = np.dot(X_train.T, dH) / n_samples\n",
    "    db1 = np.sum(dH, axis=0, keepdims=True) / n_samples\n",
    "\n",
    "    # update\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef013309",
   "metadata": {},
   "source": [
    "<h3>Step 6 — Model Evaluation</h3>\n",
    "<p>\n",
    "After training, we evaluate the network on the test set to measure how well it generalizes. \n",
    "The steps are:\n",
    "</p>\n",
    "<ol>\n",
    "  <li>\n",
    "    <strong>Forward pass on test data</strong>:\n",
    "    <ul>\n",
    "      <li>Compute hidden activations with ReLU.</li>\n",
    "      <li>Compute output probabilities with softmax.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>\n",
    "    <strong>Predictions</strong>:\n",
    "    <ul>\n",
    "      <li>Choose the class with the highest probability for each sample: <code>np.argmax(output, axis=1)</code>.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>\n",
    "    <strong>Accuracy</strong>:\n",
    "    <ul>\n",
    "      <li>Compare predicted classes with true labels.</li>\n",
    "      <li>Compute the percentage of correct predictions: <code>accuracy = correct / total</code>.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "</ol>\n",
    "<p>\n",
    "This gives us a quantitative measure (accuracy score) of how well our trained model performs \n",
    "on unseen Iris samples.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c04b3243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0616\n",
      "Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Forward pass on test data\n",
    "y_test_hidden = relu(np.dot(X_test, W1) + b1)\n",
    "y_test_pred = softmax(np.dot(y_test_hidden, W2) + b2)\n",
    "\n",
    "# Predictions: pick class with highest probability\n",
    "y_pred = np.argmax(y_test_pred, axis=1)\n",
    "y_true = y_test.flatten()\n",
    "\n",
    "# Loss\n",
    "loss = cross_entropy(y_test_oh, y_test_pred)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "\n",
    "# Accuracy\n",
    "accuracy = np.mean(y_true == y_pred)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
